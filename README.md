# ğŸ¯ Multi-Modal AI Inspector

> **Hybrid conversational AI analyst for images, audio, and video**

A production-ready full-stack application that analyzes multimedia content using state-of-the-art AI models and provides conversational insights through an intelligent chat interface.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![React](https://img.shields.io/badge/react-18.2-blue.svg)

## âœ¨ Features

### ğŸ–¼ï¸ Image Analysis
- **BLIP Captioning** - Generate natural language descriptions
- **Color Extraction** - Identify dominant color palettes
- **Tag Generation** - Automatic keyword extraction

### ğŸµ Audio Analysis
- **Whisper Transcription** - State-of-the-art speech-to-text
- **Sentiment Analysis** - Detect emotional tone
- **Keyword Extraction** - Identify key topics
- **Language Detection** - Automatic language identification

### ğŸ¬ Video Analysis
- **Frame Extraction** - Sample key moments
- **Audio Track Analysis** - Extract and transcribe audio
- **Visual Timeline** - Frame-by-frame breakdown
- **Combined Insights** - Unified analysis

### ğŸ’¬ Conversational Assistant
- **Context-Aware Chat** - Ask questions about your media
- **Structured Responses** - Facts + creative interpretation
- **Source Attribution** - Track where insights come from
- **Chat History** - Persistent conversation threads

### ğŸ¨ Modern UI
- **Dark Theme** - Sleek, professional design
- **Glassmorphism** - Modern visual effects
- **Drag & Drop** - Intuitive file uploads
- **Real-time Updates** - Live analysis progress
- **Responsive Design** - Works on all devices

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React     â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚   FastAPI    â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  AI Models  â”‚
â”‚   Frontend  â”‚  HTTP   â”‚   Backend    â”‚         â”‚   (Local)   â”‚
â”‚             â”‚         â”‚              â”‚         â”‚             â”‚
â”‚  â€¢ Upload   â”‚         â”‚  â€¢ Routes    â”‚         â”‚  â€¢ BLIP     â”‚
â”‚  â€¢ Chat UI  â”‚         â”‚  â€¢ Services  â”‚         â”‚  â€¢ Whisper  â”‚
â”‚  â€¢ Analyticsâ”‚         â”‚  â€¢ DB (SQL)  â”‚         â”‚  â€¢ LLM      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.9+**
- **Node.js 16+**
- **FFmpeg** (for video processing)
- **8GB+ RAM** (for running models locally)

### Installation

1. **Clone the repository**
```bash
git clone <your-repo-url>
cd multi-modal-ai-inspector
```

2. **Backend Setup**
```bash
cd backend
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS/Linux
source .venv/bin/activate

pip install -r requirements.txt

# Create environment file
copy .env.example .env
# Edit .env as needed

# Create storage directory
mkdir storage
```

3. **Frontend Setup**
```bash
cd ../frontend
npm install
```

### Running Locally

**Terminal 1 - Backend:**
```bash
cd backend
.venv\Scripts\activate  # or source .venv/bin/activate
python -m app.main
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
```

**Open in browser:** http://localhost:5173

## ğŸ“ Project Structure

```
multi-modal-ai-inspector/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ api/                 # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ask.py
â”‚   â”‚   â”‚   â””â”€â”€ media.py
â”‚   â”‚   â”œâ”€â”€ services/            # Analysis services
â”‚   â”‚   â”‚   â”œâ”€â”€ image_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ audio_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ video_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”‚   â””â”€â”€ orchestrator.py
â”‚   â”‚   â”œâ”€â”€ models/              # Database models
â”‚   â”‚   â”‚   â””â”€â”€ db.py
â”‚   â”‚   â””â”€â”€ utils/               # Utilities
â”‚   â”‚       â”œâ”€â”€ database.py
â”‚   â”‚       â”œâ”€â”€ file_validation.py
â”‚   â”‚       â””â”€â”€ ffmpeg.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadDropzone.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MediaPlayer.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AnalyticsPanel.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ChatPanel.jsx
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â””â”€â”€ README.md
```

## ğŸ¤– AI Models Used

| Model | Purpose | Size |
|-------|---------|------|
| [Salesforce/BLIP](https://huggingface.co/Salesforce/blip-image-captioning-base) | Image captioning | ~1GB |
| [OpenAI Whisper](https://github.com/openai/whisper) (small) | Audio transcription | ~500MB |
| [DistilBERT](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) | Sentiment analysis | ~250MB |
| [Facebook OPT-1.3B](https://huggingface.co/facebook/opt-1.3b) | Conversational LLM | ~2.5GB |

**Total model size:** ~4-5GB (downloaded automatically on first run)

## ğŸ“Š Database Schema

SQLite database with the following tables:

- `media` - Uploaded files metadata
- `analysis` - Analysis results (JSON)
- `transcript_segments` - Transcript with timestamps
- `objects` - Detected objects (future)
- `reports` - Generated summaries
- `chats` - Conversation history

## ğŸ”’ Security & Privacy

- âœ… File type and size validation
- âœ… MIME type verification
- âœ… Secure file storage
- âœ… CORS configuration
- âš ï¸ **Note:** For production, add authentication and encryption

## ğŸŒ Deployment

### Zero-Cost Options

1. **Frontend (Vercel)**
   - Push to GitHub
   - Import in Vercel
   - Auto-deploy on push

2. **Backend (Hugging Face Spaces)**
   - Create Space with FastAPI
   - Deploy backend code
   - CPU inference (free tier)

3. **Alternative: Local Demo**
   - Run locally
   - Record screencast
   - Share video demo

### Production Deployment

- **Frontend:** Vercel, Netlify, CloudFlare Pages
- **Backend:** Railway, Render, DigitalOcean
- **Database:** PostgreSQL (upgrade from SQLite)
- **Models:** GPU inference for speed

## ğŸ§ª Testing

```bash
# Backend (future)
cd backend
pytest

# Frontend (future)
cd frontend
npm test
```

## ğŸ“ˆ Roadmap

- [ ] Object detection (YOLO/DETR)
- [ ] Speaker diarization
- [ ] Multi-language support
- [ ] Batch processing
- [ ] Export reports (PDF)
- [ ] User authentication
- [ ] Cloud storage integration
- [ ] Real-time streaming analysis

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open Pull Request

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file

## ğŸ™ Acknowledgments

- **Salesforce** - BLIP image captioning
- **OpenAI** - Whisper speech recognition
- **Hugging Face** - Model hosting and transformers library
- **FFmpeg** - Media processing

## ğŸ“§ Support

For issues and questions:
- Open an issue on GitHub
- Check existing documentation
- Review API docs at `/docs` endpoint

---

**Built with â¤ï¸ using FastAPI, React, and state-of-the-art AI models**
